import requests
from bs4 import BeautifulSoup
from openai import OpenAI
from dotenv import load_dotenv
import numpy as np
import os

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def get_text_from_url(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    text = soup.get_text(separator="\n")
    return text

def build_vector_store():
    urls = [
        "https://openai.com/research",
        "https://platform.openai.com/docs/guides/retrieval",
        "https://en.wikipedia.org/wiki/OpenAI"
    ]

    documents = []
    for url in urls:
        print(f"üì• Fetching {url} ...")
        text = get_text_from_url(url)
        documents.append(text)

    if not documents:
        print("‚ö†Ô∏è Kh√¥ng c√≥ n·ªôi dung n√†o ƒë∆∞·ª£c t·∫£i v·ªÅ.")
        return

    print("‚úÖ Fetched", len(documents), "web pages.")

    texts = []
    for doc in documents:
        # C·∫Øt nh·ªè n·ªôi dung ƒë·ªÉ embedding d·ªÖ h∆°n
        chunks = [doc[i:i+1000] for i in range(0, len(doc), 1000)]
        texts.extend(chunks)

    print("Creating embeddings...")

    vectors = []
    for chunk in texts:
        emb = client.embeddings.create(
            input=chunk,
            model="text-embedding-3-small"
        )
        vectors.append(emb.data[0].embedding)

    vectors = np.array(vectors)
    print("‚úÖ Embeddings created successfully with shape:", vectors.shape)

if __name__ == "__main__":
    build_vector_store()
